{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luxadevi/Ollama-Colab-Integration/blob/main/Ollama_publicV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Instructions for Ollama API Access and NAT Tunneling\n",
        "\n",
        "This notebook provides step-by-step instructions for setting up the Ollama service with NAT tunneling. The following features are covered:\n",
        "\n",
        "- **Tunneling**: Establishing a secure connection to access the Ollama API.\n",
        "- **Background Processing**: Running Ollama and the tunnel in the background.\n",
        "- **Monitoring**: Keeping an eye on the status of Ollama and the tunnel.\n",
        "- **Logging**: Capturing all standard output (stdout) and standard error (stderr) messages from Ollama and natsrv.py.\n",
        "- **Interactive Modelfile Creator**: Creating custom Modelfiles for tailored Ollama behavior.\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "Before proceeding, please provide the required information:\n",
        "\n",
        "1. **Secret Password**: Enter the secret password for tunnel authentication.\n",
        "2. **Endpoint IP Address**: Specify the IP address for the NAT tunnel endpoint.\n",
        "\n",
        "These details are essential for the secure setup of the Ollama service and NAT tunneling.\n",
        "\n",
        "**Note**: Ensure that you have the necessary dependencies installed before following these instructions. Refer to the \"Dependency Installation\" section for guidance.\n"
      ],
      "metadata": {
        "id": "b3kluSkbkOZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "secret_pass = input(\"Please enter the secret password for --secret: \")\n",
        "admin_ip = input(\"Please enter the IP address for --admin: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2IW0XRPkNdB",
        "outputId": "5d44f709-4ab6-4e88-9c99-0918f3d638e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter the secret password for --secret: 5sdawescvf\n",
            "Please enter the IP address for --admin: 85.145.210.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Installation\n",
        "\n",
        "This script encompasses the installation of essential dependencies:\n",
        "\n",
        "- Latest CUDA drivers and toolkit.\n",
        "- Ollama.\n",
        "- Nat-tunnel configuration.\n",
        "- PCIutils for GPU information retrieval.\n",
        "- Python array-based model loading."
      ],
      "metadata": {
        "id": "oK3xOUwgV29n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4IvOEKOTJIH"
      },
      "outputs": [],
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
        "!mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n",
        "!cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
        "!apt-get update\n",
        "!apt-get -y install cuda-toolkit\n",
        "!apt-get -y install cuda\n",
        "!sudo apt install pciutils\n",
        "!lspci\n",
        "!wget https://ollama.ai/install.sh -O install.sh\n",
        "!chmod +x install.sh\n",
        "!./install.sh\n",
        "!git clone https://github.com/rofl0r/nat-tunnel.git\n",
        "!pip install httpx\n",
        "!rm rm cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n",
        "!pip install asyncio\n",
        "import requests\n",
        "# URL containing the JSON data\n",
        "url = 'https://raw.githubusercontent.com/Luxadevi/Ollama-Colab-intergration/main/models.json'\n",
        "# Fetch the JSON data from the URL\n",
        "response = requests.get(url)\n",
        "# Parse the JSON content into a Python dictionary\n",
        "models = response.json()\n",
        "# models is now a Python dictionary containing your data\n",
        "print(models)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ollama Setup and Tunnel Configuration\n",
        "\n",
        "## Tunneling Setup Instructions\n",
        "\n",
        "To get started with tunneling for the Ollama service, follow these steps:\n",
        "\n",
        "1. **Download the NAT Tunnel Script**:\n",
        "   Download the script from the GitHub repository.\n",
        "   - [Nat-Tunnel on GitHub](https://github.com/rofl0r/nat-tunnel)\n",
        "\n",
        "2. **Port Requirements**:\n",
        "   Ensure that at least one port is exposed to receive connections.\n",
        "\n",
        "3. **Server-Side Configuration**:\n",
        "   On the server side (where you receive the connection), execute the following command:\n",
        "\n",
        "   ```sh\n",
        "   python3 natsrv.py --mode server --secret s3cretP4ss --public 0.0.0.0:7000 --admin 0.0.0.0:8000\n",
        "\n",
        "**Explanation of Parameters:**\n",
        "- `--mode server`: This sets the NAT tunnel script to operate in server mode.\n",
        "- `--secret s3cretP4ss`: A customizable secret code. This code will be used for authentication and should also be provided when prompted in this notebook.\n",
        "- `--public 0.0.0.0:7000`: Defines port 7000 as the public-facing port for Ollama. This port is necessary for remote access; omit this if you only need local access.\n",
        "- `--admin 0.0.0.0:8000`: Port 8000 is dedicated to administrative controls. Ensure this port is forwarded for remote management capabilities.\n"
      ],
      "metadata": {
        "id": "wgICJof_TVUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import logging.handlers\n",
        "import httpx\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def create_logger(name, filename, level, formatter):\n",
        "    logger = logging.getLogger(name)\n",
        "    handler = logging.handlers.RotatingFileHandler(filename, maxBytes=5*1024*1024, backupCount=5)\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n",
        "    logger.setLevel(level)\n",
        "    return logger\n",
        "\n",
        "status_formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] [%(name)s] - %(message)s')\n",
        "error_formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] [%(name)s] - %(message)s')\n",
        "\n",
        "loggers = {\n",
        "    \"Status\": create_logger(\"Status\", \"status.log\", logging.INFO, status_formatter),\n",
        "    \"NatsrvStatus\": create_logger(\"NatsrvStatus\", \"natsrv.log\", logging.INFO, status_formatter),\n",
        "    \"OllamaStatus\": create_logger(\"OllamaStatus\", \"ollama.log\", logging.INFO, status_formatter),\n",
        "    \"Error\": create_logger(\"Error\", \"error.log\", logging.ERROR, error_formatter),\n",
        "    \"NatsrvError\": create_logger(\"NatsrvError\", \"natsrv_error.log\", logging.ERROR, error_formatter),\n",
        "    \"OllamaError\": create_logger(\"OllamaError\", \"ollama_error.log\", logging.ERROR, error_formatter)\n",
        "}\n",
        "\n",
        "class ProcessMonitor:\n",
        "    def __init__(self):\n",
        "        self.processes = {}\n",
        "        self.is_monitoring = True\n",
        "\n",
        "    def handle_output(self, process_name):\n",
        "        process = self.processes[process_name]\n",
        "        logger_status = loggers[f\"{process_name.capitalize()}Status\"]\n",
        "        for line in iter(process.stdout.readline, b''):\n",
        "            logger_status.info(line.decode().strip())\n",
        "\n",
        "    def check_url_and_restart_natsrv(self):\n",
        "        while self.is_monitoring:\n",
        "            try:\n",
        "                response = httpx.get(f\"http://{admin_ip}:7000/\")\n",
        "                if response.status_code != 200:\n",
        "                    raise Exception(\"Non-200 status code\")\n",
        "            except Exception as e:\n",
        "                loggers[\"Error\"].error(f\"Error accessing the URL: {e}. Restarting natsrv.py...\")\n",
        "                if self.processes.get('natsrv'):\n",
        "                    self.processes['natsrv'].terminate()\n",
        "                self.run_natsrv()\n",
        "            time.sleep(5)\n",
        "\n",
        "    def run_natsrv(self):\n",
        "        cmd = f\"python3 /content/nat-tunnel/natsrv.py --mode client --secret {secret_pass} --local localhost:11434 --admin {admin_ip}:8000\"\n",
        "        # Redirect subprocess output to /dev/null\n",
        "        with open(os.devnull, 'wb') as devnull:\n",
        "            self.processes['natsrv'] = subprocess.Popen(cmd, shell=True, stdout=devnull, stderr=devnull)\n",
        "        loggers[\"NatsrvStatus\"].info(f\"Started natsrv with command: {cmd}\")\n",
        "\n",
        "    def run_ollama(self):\n",
        "        os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n",
        "        os.environ[\"OLLAMA_ORIGINS\"] = \"http://0.0.0.0:*\"\n",
        "\n",
        "        cmd = \"ollama serve\"\n",
        "        # Redirect subprocess output to /dev/null\n",
        "        with open(os.devnull, 'wb') as devnull:\n",
        "            self.processes['ollama'] = subprocess.Popen(cmd, shell=True, stdout=devnull, stderr=devnull)\n",
        "        loggers[\"OllamaStatus\"].info(f\"Started ollama with command: {cmd}\")\n",
        "\n",
        "\n",
        "    def monitor_process(self, process_name):\n",
        "        while self.is_monitoring:\n",
        "            if self.processes[process_name].poll() is not None:\n",
        "                loggers[\"Status\"].warning(f\"{process_name} process has stopped. Restarting...\")\n",
        "                if process_name == 'natsrv':\n",
        "                    self.run_natsrv()\n",
        "                else:\n",
        "                    self.run_ollama()\n",
        "            time.sleep(5)\n",
        "\n",
        "    def start(self):\n",
        "        self.run_ollama()\n",
        "        time.sleep(2)\n",
        "        self.run_natsrv()\n",
        "\n",
        "        threading.Thread(target=self.monitor_process, args=('ollama',)).start()\n",
        "        threading.Thread(target=self.monitor_process, args=('natsrv',)).start()\n",
        "        threading.Thread(target=self.check_url_and_restart_natsrv).start()\n",
        "\n",
        "    def stop(self):\n",
        "        self.is_monitoring = False\n",
        "        for p in self.processes.values():\n",
        "            p.terminate()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    monitor = ProcessMonitor()\n",
        "    monitor.start()\n"
      ],
      "metadata": {
        "id": "kGOANlieUlRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Modelfile Maker\n",
        "\n",
        "## Overview\n",
        "Create your own modelfile with ease using this intuitive tool. Tailor it according to your needs, choose your model and model type, and get started in no time!\n",
        "\n",
        "### Features\n",
        "- **Model Selection**: Pick the model that fits your requirements.\n",
        "- **Modeltype Customization**: Select from various available model types.\n",
        "- **Naming**: Input fields for naming your model determine how the API will identify it.\n",
        "- **Parameterization**: Flexibility to use specific PARAMETERS or opt not to use any.\n",
        "- **Template Variables**: Add custom template variables or choose not to include any.\n",
        "\n",
        "### Disclaimer\n",
        "When generating and deploying, please note that it might take some time before you see the output. For quicker feedback, you can use the following `curl` command:\n",
        "\n",
        "```sh\n",
        "curl -X POST http://127.0.0.1:7000/api/create -d '{\n",
        "  \"name\": \"modelname\",\n",
        "  \"path\": \"/content/Modelfile\"\n",
        "}'\n",
        "\n"
      ],
      "metadata": {
        "id": "-Plt9SOw02gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, FileLink\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Sample models dictionary\n",
        "\n",
        "parameters = {\n",
        "    'mirostat': 'Enable mirostat (default: false)',\n",
        "    'mirostat_eta': 'Mirostat eta (default: 0.1)',\n",
        "    'mirostat_tau': 'Mirostat tau (default: 0.1)',\n",
        "    'num_ctx': 'Number of tokens of context to use (default: 4096)',\n",
        "    'num_gqa': 'Number of tokens to generate per request (default: 256)',\n",
        "    'num_gpu': 'Number of GPUs to use (default: 1)',\n",
        "    'num_thread': 'Number of threads to use (default: 1)',\n",
        "    'repeat_last_n': 'Repeat last n tokens of input (default: 0)',\n",
        "    'repeat_penalty': 'Repeat penalty (default: 1.0)',\n",
        "    'temperature': 'Sampling temperature (default: 0.8)',\n",
        "    'seed': 'Random seed',\n",
        "    'stop': 'Stop sequence for generation',\n",
        "    'tfs_z': 'Enable TFS z (default: false)',\n",
        "    'num_predict': 'Number of tokens to generate (default: 256)',\n",
        "    'top_k': 'Top-k sampling (default: 0)',\n",
        "    'top_p': 'Top-p sampling (default: 1.0)'\n",
        "}\n",
        "# Create a dropdown for model selection\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=models.keys(),\n",
        "    description='Model:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Input field for the name of the Modelfile\n",
        "modelfile_name_input = widgets.Text(value='', placeholder='Enter Modelfile name', description='Modelfile Name:', layout=widgets.Layout(width='300px'))\n",
        "\n",
        "# Input field for the name in the data variable\n",
        "data_name_input = widgets.Text(value='', placeholder='Enter name for data', description='Name :', layout=widgets.Layout(width='300px'))\n",
        "\n",
        "# Create a dropdown for model type based on selected model\n",
        "model_type_dropdown = widgets.Dropdown(\n",
        "    options=models[model_dropdown.value],\n",
        "    description='Model Type:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "def update_model_type_options(change):\n",
        "    model_type_dropdown.options = models[change['new']]\n",
        "\n",
        "model_dropdown.observe(update_model_type_options, names='value')\n",
        "\n",
        "# Create checkboxes for PARAMETERS with input fields showing the description\n",
        "checkboxes = []\n",
        "input_fields = []\n",
        "for param, desc in parameters.items():\n",
        "    checkbox = widgets.Checkbox(value=False, description=param)\n",
        "    input_field = widgets.Text(value='', placeholder=desc)\n",
        "    checkboxes.append(checkbox)\n",
        "    input_fields.append(input_field)\n",
        "\n",
        "# Function to generate and save Modelfile\n",
        "def generate_modelfile(btn=None):\n",
        "    modelfile_content = f\"FROM {model_dropdown.value}:{model_type_dropdown.value}\\n\"\n",
        "\n",
        "    for checkbox, input_field in zip(checkboxes, input_fields):\n",
        "        if checkbox.value:\n",
        "            modelfile_content += f\"PARAMETER {checkbox.description} {input_field.value}\\n\"\n",
        "\n",
        "    system_value = template_input_fields[0].value.strip()\n",
        "    prompt_value = template_input_fields[1].value.strip()\n",
        "    first_value = template_input_fields[2].value.strip()\n",
        "\n",
        "    if system_value or prompt_value or first_value:\n",
        "        modelfile_content += 'TEMPLATE \"\"\"\\n'\n",
        "        if first_value:\n",
        "            modelfile_content += \"{{- if .First }}\\n\"\n",
        "            modelfile_content += \"### System:\\n\"\n",
        "            modelfile_content += f\"{{ {system_value} }}\\n\"\n",
        "            modelfile_content += \"{{- end }}\\n\"\n",
        "        modelfile_content += '\"\"\"'\n",
        "\n",
        "    filename = modelfile_name_input.value\n",
        "    with open(filename, \"w\") as file:\n",
        "        file.write(modelfile_content)\n",
        "\n",
        "    if btn:  # Only display the link if the function was called by a button click\n",
        "        display(FileLink(filename))\n",
        "\n",
        "# Function to generate, save, and deploy Modelfile\n",
        "def generate_and_deploy(btn):\n",
        "    generate_modelfile()  # Generate and save the Modelfile\n",
        "\n",
        "    data = {\n",
        "        \"name\": data_name_input.value,\n",
        "        \"path\": f\"/content/{modelfile_name_input.value}\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(\"http://localhost:11434/api/create\", json=data, headers={\"Content-Type\": \"application/json\"})\n",
        "    print(response.text)  # Display the response in the notebook output\n",
        "\n",
        "# Button to generate Modelfile\n",
        "generate_button = widgets.Button(description=\"Generate Modelfile\")\n",
        "generate_button.on_click(generate_modelfile)\n",
        "\n",
        "# Button to generate and deploy Modelfile\n",
        "deploy_button = widgets.Button(description=\"Generate and Deploy\")\n",
        "deploy_button.on_click(generate_and_deploy)\n",
        "\n",
        "# Update descriptions for Template Variables Input Fields\n",
        "template_input_fields = [\n",
        "    widgets.Text(value='', placeholder='', description='System:'),\n",
        "    widgets.Text(value='', placeholder='', description='Prompt:'),\n",
        "    widgets.Text(value='', placeholder='', description='First:')\n",
        "]\n",
        "\n",
        "template_input_fields[0].placeholder = \"The system prompt used to specify custom behavior. This must also be set in the Modelfile as an instruction.\"\n",
        "template_input_fields[1].placeholder = \"The incoming prompt. This is not specified in the model file and will be set based on input.\"\n",
        "template_input_fields[2].placeholder = \"A boolean value used to render specific template information for the first generation of a session.\"\n",
        "\n",
        "template_label = widgets.Label(value=\"Template Variables\")\n",
        "template_container = widgets.VBox(template_input_fields)\n",
        "\n",
        "# Display widgets\n",
        "display(modelfile_name_input)\n",
        "display(data_name_input)  # Display the new input field\n",
        "display(model_dropdown)\n",
        "display(model_type_dropdown)\n",
        "\n",
        "for checkbox, input_field in zip(checkboxes, input_fields):\n",
        "    display(widgets.HBox([checkbox, input_field]))\n",
        "\n",
        "# Display Template Variables Input Fields\n",
        "display(template_label)\n",
        "display(template_container)\n",
        "\n",
        "display(widgets.HBox([generate_button, deploy_button]))\n"
      ],
      "metadata": {
        "id": "E0j0rMvPTmtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful commands"
      ],
      "metadata": {
        "id": "kvhDpCOvvPy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hard kill everything\n",
        "\n",
        "\n",
        "Kill all instances of the background procceses"
      ],
      "metadata": {
        "id": "brgf_DBFvTMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f \"python3 /content/nat-tunneling/natsrv.py\"\n",
        "!pkill -f \"ollama serve\"\n",
        "monitor.stop()"
      ],
      "metadata": {
        "id": "83mJB5BVVALp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example local model create and generate promt for quick swithing of models"
      ],
      "metadata": {
        "id": "tCEWoSJa2Bvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "curl -X POST http://127.0.0.1:11434/api/create -d '{\n",
        "  \"name\": \"modelname\",\n",
        "  \"path\": \"/content/Modelfile\"\n",
        "}'"
      ],
      "metadata": {
        "id": "pSXHKdRizmS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "curl -X POST http://127.0.0.1:11434/api/generate -d '{\n",
        "  \"model\": \"ne\",\n",
        "  \"prompt\":\"Why is the sky blue?\"\n",
        "}'"
      ],
      "metadata": {
        "id": "sJQvrJQU5sOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experience and behavior\n",
        "\n",
        "Through testing, I've noticed that loading certain models onto the GPU can be challenging and may occasionally lead to crashes. A practical workaround involves initially creating a small, dummy model. This strategy allows for the quick unloading of any problematic models, followed by another attempt with a larger one. It's important to note that if a model loads successfully after a crash, it will operate using only the CPU. At this juncture, you should load the small model and then retry loading the larger one.\n",
        "\n",
        "A critical point to remember: avoid exceeding 13GB of VRAM usage. Surpassing this limit tends to overheat the system, leading to crashes.\n",
        "\n",
        "These issues often stem from insufficient RAM or storage capacity required to preload the model before transferring it to the GPU.\n",
        "\n",
        "For enhanced performance at no extra cost, consider using Kaggle, which offers up to 24GB VRAM and additional RAM. For different setups and more information, check out the Kaggle version on my Github.\n",
        "\n"
      ],
      "metadata": {
        "id": "o1Qr7wKy2-bN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "* Add dynamic viewing of logging\n",
        "* More functions for ollama API\n"
      ],
      "metadata": {
        "id": "Y1SV6bu8Ax-b"
      }
    }
  ]
}