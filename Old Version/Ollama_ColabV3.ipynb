{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "0z6YVvZyLu5w",
        "kQtpWcTqPxpy"
      ],
      "authorship_tag": "ABX9TyNyY2JfJmqsDkuMI6p/9/Wm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luxadevi/Ollama-Colab-Integration/blob/main/Ollama_ColabV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***OLLAMA COLAB INTEGRATION V3***\n",
        "---\n",
        "Seamless integration of Ollama, offering fully autonomous public endpoints. No need for additional software.\n",
        "\n",
        "## Now with the awaited Ollama Companion\n",
        "\n",
        "Build with gradio, Ollama companion is a web interface to make it easier to quickly manage Ollama.\n",
        "\n",
        "\n",
        "### ***Ollama Companion provides following features***\n",
        "\n",
        "* Modelfilde builder\n",
        "* Dropdowns and information about all current models\n",
        "* Modelfile viewer\n",
        "* Model listing and overview\n",
        "* Public endpoint with cloudflare\n",
        "* Litellm proxy with polling of model list\n",
        "* Adds models to LiteLLM config.yaml when creating models and restarts LiteLLM if running\n",
        "* Proper logging for all services\n",
        "* Up to-Date Model list from ollama 28-11-2023"
      ],
      "metadata": {
        "id": "gRkIzPgbK2lr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cUKOkICK2Gq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Installing Dependencies and Updates\n",
        "# @markdown Check the box below to install CUDA 12-3 before proceeding.\n",
        "# @markdown\n",
        "# @markdown This CUDA installation typically takes around 5 minutes to complete and does not require a system reboot.\n",
        "# @markdown The latest version of the code only needs this cell to run, but you can refer to the code below for reference.\n",
        "# @markdown It initiates the Ollama serve process and deploys the Ollama Companion from the 'tools' directory.\n",
        "\n",
        "install_cuda = False #@param {type:\"boolean\"}\n",
        "# Define the log file path\n",
        "log_file_path = \"/content/install.log\"\n",
        "\n",
        "# Function to run a command and log its output\n",
        "def run_command(command):\n",
        "    description = \" \".join(command.split()[1:])  # Use the entire command as the description\n",
        "    print(f\"Installing {description}...\")\n",
        "    with open(log_file_path, \"a\") as log_file:\n",
        "        log_file.write(f\"Installing {description}...\\n\")\n",
        "        log_file.flush()  # Flush the buffer to ensure immediate writing to the log file\n",
        "\n",
        "    # Run the command and capture its output (stdout and stderr)\n",
        "    output = !{command}  # Capture the command output (stdout and stderr)\n",
        "    with open(log_file_path, \"a\") as log_file:\n",
        "        log_file.write(\"\\n\".join(output) + \"\\n\")\n",
        "        log_file.flush()  # Flush the buffer to ensure immediate writing to the log file\n",
        "\n",
        "    print(f\"Successfully installed {description}.\")\n",
        "\n",
        "if install_cuda:\n",
        "    # CUDA installation commands\n",
        "    cuda_commands = [\n",
        "        \"wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\",\n",
        "        \"mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\",\n",
        "        \"wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\",\n",
        "        \"dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\",\n",
        "        \"cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\",\n",
        "        \"apt-get update\",\n",
        "        \"apt-get -y install cuda-toolkit\",\n",
        "        \"apt-get -y install cuda\",\n",
        "    ]\n",
        "\n",
        "    for cmd in cuda_commands:\n",
        "        run_command(cmd)\n",
        "\n",
        "    print(\"CUDA 12-3 installed successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"CUDA installation skipped.\")\n",
        "\n",
        "# Other commands\n",
        "other_commands = [\n",
        "    \"sudo apt install pciutils\",\n",
        "    \"lspci\",\n",
        "    \"wget https://ollama.ai/install.sh -O install.sh\",\n",
        "    \"chmod +x install.sh\",\n",
        "    \"./install.sh\",\n",
        "    \"pip install asyncio flask_cloudflared gradio requests PyYAML > pip.log 2>&1\",  # Redirect both stdout and stderr to pip.log\n",
        "    \"pip install -U litellm > litellm.log 2>&1\",  # Redirect both stdout and stderr to litellm.log\n",
        "    \"git clone https://github.com/Luxadevi/Ollama-Colab-Integration.git /content/Ollama-Colab-Integration\",\n",
        "    \"mv /content/Ollama-Colab-Integration/* /content/\",\n",
        "    \"rm -rf /content/Ollama-Colab-Integration\",\n",
        "]\n",
        "\n",
        "for cmd in other_commands:\n",
        "    run_command(cmd)\n",
        "\n",
        "# Start ollama.py in the background\n",
        "print(\"Starting ollama.py in the background...\")\n",
        "!nohup python3 /content/tools/ollama.py &\n",
        "\n",
        "# Run ollama_companion.py\n",
        "print(\"Running ollama_companion.py...\")\n",
        "print(\"Public Endpoint Available in Companion Webui\")\n",
        "!python3 /content/tools/ollama_companion.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Ollama in Subprocess"
      ],
      "metadata": {
        "id": "0z6YVvZyLu5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import logging.handlers\n",
        "import httpx\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def create_logger(name, filename, level, formatter):\n",
        "    logger = logging.getLogger(name)\n",
        "    handler = logging.handlers.RotatingFileHandler(filename, maxBytes=5*1024*1024, backupCount=5)\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n",
        "    logger.setLevel(level)\n",
        "    return logger\n",
        "\n",
        "status_formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] [%(name)s] - %(message)s')\n",
        "error_formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] [%(name)s] - %(message)s')\n",
        "\n",
        "loggers = {\n",
        "    \"Status\": create_logger(\"Status\", \"status.log\", logging.INFO, status_formatter),\n",
        "    \"OllamaStatus\": create_logger(\"OllamaStatus\", \"ollama.log\", logging.INFO, status_formatter),\n",
        "    \"Error\": create_logger(\"Error\", \"error.log\", logging.ERROR, error_formatter),\n",
        "    \"OllamaError\": create_logger(\"OllamaError\", \"ollama_error.log\", logging.ERROR, error_formatter)\n",
        "}\n",
        "\n",
        "class ProcessMonitor:\n",
        "    def __init__(self):\n",
        "        self.processes = {}\n",
        "        self.is_monitoring = True\n",
        "\n",
        "    def handle_output(self, process_name):\n",
        "        process = self.processes[process_name]\n",
        "        logger_status = loggers[f\"{process_name.capitalize()}Status\"]\n",
        "        for line in iter(process.stdout.readline, b''):\n",
        "            logger_status.info(line.decode().strip())\n",
        "\n",
        "    def run_ollama(self):\n",
        "        os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n",
        "        os.environ[\"OLLAMA_ORIGINS\"] = \"http://0.0.0.0:*\"\n",
        "\n",
        "        cmd = \"ollama serve\"\n",
        "        # Redirect subprocess output to /dev/null\n",
        "        with open(os.devnull, 'wb') as devnull:\n",
        "            self.processes['ollama'] = subprocess.Popen(cmd, shell=True, stdout=devnull, stderr=devnull)\n",
        "        loggers[\"OllamaStatus\"].info(f\"Started ollama with command: {cmd}\")\n",
        "\n",
        "    def monitor_process(self, process_name):\n",
        "        while self.is_monitoring:\n",
        "            if self.processes[process_name].poll() is not None:\n",
        "                loggers[\"Status\"].warning(f\"{process_name} process has stopped. Restarting...\")\n",
        "                self.run_ollama()\n",
        "            time.sleep(5)\n",
        "\n",
        "    def start(self):\n",
        "        self.run_ollama()\n",
        "        threading.Thread(target=self.monitor_process, args=('ollama',)).start()\n",
        "\n",
        "    def stop(self):\n",
        "        self.is_monitoring = False\n",
        "        for p in self.processes.values():\n",
        "            p.terminate()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    monitor = ProcessMonitor()\n",
        "    monitor.start()\n"
      ],
      "metadata": {
        "id": "QXXLAUHPLGs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ollama Companion"
      ],
      "metadata": {
        "id": "kQtpWcTqPxpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import subprocess\n",
        "import requests\n",
        "import re  # Import the re module\n",
        "from flask import Flask, request, Response\n",
        "from flask_cloudflared import run_with_cloudflared\n",
        "from threading import Thread\n",
        "import time\n",
        "import yaml\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "litellm_proxycmd = \"PYTHONUNBUFFERED=1 litellm --config ./config.yaml >> litellmlog 2>&1 &\"\n",
        "\n",
        "polling_active = False\n",
        "endpointcmd = \"PYTHONUNBUFFERED=1 python3 /content/endpoint.py >> endpoint.log 2>&1 &\"\n",
        "kill_endpointcmd = \"pkill -f '/content/endpoint.py'\"\n",
        "\n",
        "\n",
        "# Global variables for dropdown options\n",
        "option_1_global = None\n",
        "option_2_global = None\n",
        "cloudflare_url = None\n",
        "# Fetching models data from the URL\n",
        "url = \"https://raw.githubusercontent.com/Luxadevi/Ollama-Colab-Integration/main/models.json\"\n",
        "response = requests.get(url)\n",
        "json_data = response.json()\n",
        "\n",
        "# Structuring the data\n",
        "options_1 = list(json_data.keys())  # ['mistral', 'llama2', 'codellama', ...]\n",
        "options_2 = json_data  # The entire JSON data\n",
        "\n",
        "# Parameters with their default values and ranges\n",
        "parameters = {\n",
        "    'mirostat': [0, [0, 1, 2]],  # Dropdown\n",
        "    'mirostat_eta': [0.1, (0.0, 1.0)],\n",
        "    'mirostat_tau': [0.1, (0.0, 1.0)],\n",
        "    'num_ctx': [4096, (1024, 8192)],\n",
        "    'num_gqa': [256, (128, 512)],\n",
        "    'num_gpu': [1, (1, 250)],\n",
        "    'num_thread': [1, (1, 30)],\n",
        "    'repeat_last_n': [0, (0, 32000)],\n",
        "    'repeat_penalty': [1.0, (0.5, 2.0)],\n",
        "    'temperature': [0.8, (0.1, 1.0)],\n",
        "    'seed': [None, (0, 10000)],  # None indicates no default value\n",
        "    'tfs_z': [1, (1, 20)],  # Slider from 1 to 20\n",
        "    'num_predict': [256, (128, 512)],\n",
        "    'top_k': [0, (0, 100)],\n",
        "    'top_p': [1.0, (0.1, 1.0)]\n",
        "}\n",
        "\n",
        "def initialize_log_files():\n",
        "    log_files = [\"litellmlog\", \"endpoint.log\", \"endpoint_openai.log\"]\n",
        "    content_dir = '/content/'  # Directory path to /content/\n",
        "\n",
        "    for log_file in log_files:\n",
        "        log_file_path = os.path.join(content_dir, log_file)\n",
        "\n",
        "        if not os.path.exists(log_file_path):\n",
        "            open(log_file_path, 'w').close()\n",
        "            print(f\"Created log file: {log_file_path}\")\n",
        "        else:\n",
        "            print(f\"Log file already exists: {log_file_path}\")\n",
        "\n",
        "def is_litellm_running():\n",
        "    \"\"\"Check if LiteLLM is currently running.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\"pgrep\", \"-f\", \"litellm --config\"], capture_output=True, text=True)\n",
        "        return result.stdout != \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking if LiteLLM is running: {e}\")\n",
        "        return False\n",
        "\n",
        "def restart_litellm():\n",
        "    \"\"\"Restart the LiteLLM process.\"\"\"\n",
        "    try:\n",
        "        kill_litellm_proxy()\n",
        "        time.sleep(5)\n",
        "        start_litellm_proxy_and_read_log()\n",
        "        print(\"LiteLLM proxy restarted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error restarting LiteLLM: {e}\")\n",
        "\n",
        "def kill_litellm_proxy():\n",
        "    try:\n",
        "        # Command to kill the LiteLLM proxy process\n",
        "        kill_command = \"pkill -f 'litellm --config'\"\n",
        "\n",
        "        # Execute the kill command\n",
        "        os.system(kill_command)\n",
        "\n",
        "        return \"LiteLLM proxy process terminated.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "def start_litellm_proxy_and_read_log():\n",
        "    try:\n",
        "        # Start the LiteLLM proxy using subprocess\n",
        "        subprocess.Popen(litellm_proxycmd, shell=True)\n",
        "\n",
        "        # Wait for some time for the proxy to start and log\n",
        "        time.sleep(15)\n",
        "\n",
        "        # Read the log file and search for specific lines\n",
        "        log_file_path = \"/content/litellmlog\"\n",
        "        with open(log_file_path, \"r\") as log_file:\n",
        "            lines = log_file.readlines()\n",
        "\n",
        "        # Find and return the relevant lines\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"LiteLLM: Proxy initialized with Config, Set models:\" in line:\n",
        "                # Assuming the model names are listed in the following lines\n",
        "                model_lines = [lines[i + j].strip() for j in range(1, len(lines) - i) if lines[i + j].strip()]\n",
        "                return \"\\n\".join([line.strip()] + model_lines)\n",
        "\n",
        "        return \"Relevant log information not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def poll_api():\n",
        "    global polling_active\n",
        "    while polling_active:\n",
        "        response = requests.get(\"http://127.0.0.1:11434/api/tags\")\n",
        "        if response.status_code == 200:\n",
        "            json_data = response.json()\n",
        "            model_names = [model['name'] for model in json_data.get('models', [])]\n",
        "            update_config_file(model_names)\n",
        "        time.sleep(15)\n",
        "\n",
        "def start_polling():\n",
        "    global polling_active\n",
        "    polling_active = True\n",
        "    threading.Thread(target=poll_api).start()\n",
        "    return \"Polling started\"\n",
        "\n",
        "def stop_polling():\n",
        "    global polling_active\n",
        "    polling_active = False\n",
        "    return \"Polling stopped\"\n",
        "def update_config_file(model_names):\n",
        "    config_file_path = \"/content/config.yaml\"\n",
        "\n",
        "    # Read the existing content of the config file\n",
        "    with open(config_file_path, \"r\") as file:\n",
        "        try:\n",
        "            config = yaml.safe_load(file) or {}\n",
        "        except yaml.YAMLError as e:\n",
        "            print(f\"Error reading config file: {e}\")\n",
        "            return\n",
        "\n",
        "    # Ensure 'model_list' key exists in the configuration\n",
        "    if 'model_list' not in config:\n",
        "        config['model_list'] = []\n",
        "\n",
        "    existing_models = {model['model_name'] for model in config['model_list']}\n",
        "    needs_update = False\n",
        "\n",
        "    # Update the 'model_list' with new models\n",
        "    for model_name in model_names:\n",
        "        full_model_name = f\"ollama/{model_name}\"\n",
        "        if full_model_name not in existing_models:\n",
        "            entry = {\n",
        "                'model_name': full_model_name,\n",
        "                'litellm_params': {\n",
        "                    'model': full_model_name,\n",
        "                    'api_base': \"http://127.0.0.1:11434\",\n",
        "                    'json': True\n",
        "                }\n",
        "            }\n",
        "            config['model_list'].append(entry)\n",
        "            existing_models.add(full_model_name)\n",
        "            needs_update = True\n",
        "\n",
        "    # Write the updated content back to the YAML file and restart LiteLLM if necessary\n",
        "    if needs_update:\n",
        "        with open(config_file_path, \"w\") as file:\n",
        "            yaml.dump(config, file, default_flow_style=False, sort_keys=False)\n",
        "        if is_litellm_running():\n",
        "            restart_litellm()\n",
        "\n",
        "\n",
        "def start_openai_proxy():\n",
        "    try:\n",
        "        # Specify the command to start the OpenAI proxy endpoint\n",
        "        openai_endpointcmd = \"PYTHONUNBUFFERED=1 python3 /content/endpointopenai.py >> endpoint_openai.log 2>&1 &\"\n",
        "\n",
        "        # Start the OpenAI proxy endpoint\n",
        "        subprocess.Popen(openai_endpointcmd, shell=True)\n",
        "\n",
        "        # Wait for 15 seconds (adjust as needed)\n",
        "        time.sleep(15)\n",
        "\n",
        "        # Read the last 2 lines from the endpoint_openai.log file\n",
        "        log_file_path = \"/content/endpoint_openai.log\"\n",
        "        with open(log_file_path, \"r\") as log_file:\n",
        "            lines = log_file.readlines()\n",
        "            last_2_lines = \"\".join(lines[-2:])  # Concatenate the last 2 lines\n",
        "\n",
        "        return last_2_lines\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "def start_endpoint_and_get_last_2_lines():\n",
        "    try:\n",
        "        # Start the Flask endpoint (use subprocess.Popen as before)\n",
        "        subprocess.Popen(endpointcmd, shell=True)\n",
        "\n",
        "        # Wait for 15 seconds (adjust as needed)\n",
        "        time.sleep(15)\n",
        "\n",
        "        # Read the last 2 lines from the endpoint.log file\n",
        "        log_file_path = \"/content/endpoint.log\"\n",
        "        with open(log_file_path, \"r\") as log_file:\n",
        "            lines = log_file.readlines()\n",
        "            last_2_lines = \"\".join(lines[-2:])  # Concatenate the last 2 lines\n",
        "\n",
        "        return last_2_lines\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "def kill_endpoint():\n",
        "    try:\n",
        "        # Specify the commands to kill both processes\n",
        "        kill_endpointcmd = \"pkill -f '/content/endpoint.py'\"\n",
        "        kill_openai_endpointcmd = \"pkill -f '/content/endpointopenai.py'\"\n",
        "\n",
        "        # Execute the kill commands for both processes\n",
        "        os.system(kill_endpointcmd)\n",
        "        os.system(kill_openai_endpointcmd)\n",
        "\n",
        "        return \"Endpoints killed successfully.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "def build_curl_command(model_name, modelfile_content, stop_sequence, *args):\n",
        "    try:\n",
        "        # Check if 'FROM' is present in the modelfile_content\n",
        "        if 'FROM' not in modelfile_content:\n",
        "            modelfile_content = f\"FROM {option_1_global}:{option_2_global}\" + modelfile_content\n",
        "\n",
        "        for param, value in zip(parameters.keys(), args):\n",
        "            default = parameters[param][0]\n",
        "            if value != default:\n",
        "                if param == 'mirostat':\n",
        "                    modelfile_content += f\"\\nPARAMETER {['disabled', 'Mirostat 1', 'Mirostat 2.0'][value]}\"\n",
        "                else:\n",
        "                    modelfile_content += f\"\\nPARAMETER {param} {value}\"\n",
        "\n",
        "        if stop_sequence:  # Add stop sequence if provided\n",
        "            modelfile_content += f\"\\nPARAMETER stop {stop_sequence}\"\n",
        "\n",
        "        data = {\n",
        "            \"name\": model_name,\n",
        "            \"modelfile\": modelfile_content\n",
        "        }\n",
        "        curl_command = f\"curl http://localhost:11434/api/create -d '{json.dumps(data)}'\"\n",
        "        process = subprocess.run(curl_command, shell=True, capture_output=True, text=True)\n",
        "        return curl_command, process.stdout or process.stderr\n",
        "    except Exception as e:\n",
        "        return \"\", f\"Error: {str(e)}\"\n",
        "def create_model_manually(model_name, modelfile_content, stream_response):\n",
        "    try:\n",
        "        data = {\n",
        "            \"name\": model_name,\n",
        "            \"modelfile\": modelfile_content,\n",
        "            \"stream\": stream_response\n",
        "        }\n",
        "        response = requests.post(\"http://localhost:11434/api/create\", json=data)\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        return {\"curl_command\": \"\", \"execution_output\": f\"Error: {str(e)}\"}\n",
        "def show_model_details(model_name):\n",
        "    data = {\"name\": model_name}\n",
        "    curl_command = f\"curl http://localhost:11434/api/show -d '{json.dumps(data)}'\"\n",
        "    process = subprocess.run(curl_command, shell=True, capture_output=True, text=True)\n",
        "    output = process.stdout or process.stderr\n",
        "\n",
        "    try:\n",
        "        # Parse the JSON data\n",
        "        json_data = json.loads(output)\n",
        "\n",
        "        # Extracting the specific keys\n",
        "        license_info = json_data.get('license', 'Not available')\n",
        "        modelfile_info = json_data.get('modelfile', 'Not available')\n",
        "        parameters_info = json.dumps(json_data.get('parameters', {}), indent=4)\n",
        "        template_info = json_data.get('template', 'Not available')\n",
        "\n",
        "        return license_info, modelfile_info, parameters_info, template_info\n",
        "    except json.JSONDecodeError:\n",
        "        # Return a tuple with error message if it's not valid JSON\n",
        "        return (output, \"\", \"\", \"\")\n",
        "def list_models():\n",
        "    url = \"http://127.0.0.1:11434/api/tags\"\n",
        "    response = requests.get(url)\n",
        "    models = response.json().get('models', [])\n",
        "    return \"\\n\".join([model['name'] for model in models])\n",
        "def create_model_manually(model_name, modelfile_content, stream_response):\n",
        "    try:\n",
        "        data = {\n",
        "            \"name\": model_name,\n",
        "            \"modelfile\": modelfile_content,\n",
        "            \"stream\": stream_response\n",
        "        }\n",
        "        response = requests.post(\"http://localhost:11434/api/create\", json=data)\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        return {\"curl_command\": \"\", \"execution_output\": f\"Error: {str(e)}\"}\n",
        "\n",
        "def main():\n",
        "    initialize_log_files()\n",
        "    with gr.Blocks(theme='ParityError/LimeFace') as app:\n",
        "\n",
        "        gr.Markdown(\"Ollama Companion\")\n",
        "\n",
        "        with gr.Tab(\"ModelFile Templater\"):\n",
        "            with gr.Row():\n",
        "                model_name = gr.Textbox(label=\"Model Name\", placeholder=\"Enter model name\")\n",
        "                modelfile_content_input = gr.Textbox(lines=10, label=\"Modelfile Content\", placeholder=\"Enter manual modelfile content\")\n",
        "                stop_sequence = gr.Textbox(label=\"Stop Sequence\", placeholder=\"Enter stop sequence\")\n",
        "                d1 = gr.Dropdown(choices=options_1, label=\"Model-Provider\")\n",
        "                d2 = gr.Dropdown([])\n",
        "\n",
        "                def update_second(first_val):\n",
        "                    d2 = gr.Dropdown(options_2[first_val])\n",
        "                    return d2\n",
        "\n",
        "                d1.input(update_second, d1, d2)\n",
        "\n",
        "                outputs = gr.Textbox()\n",
        "\n",
        "                def print_results(option_1, option_2):\n",
        "                    global option_1_global, option_2_global  # Declare them as global\n",
        "                    option_1_global = option_1  # Update global variable\n",
        "                    option_2_global = option_2  # Update global variable\n",
        "                    return f\"You selected '{option_1}:{option_2}' in the second dropdown.\"\n",
        "\n",
        "                d2.input(print_results, [d1, d2], outputs)\n",
        "\n",
        "            parameter_inputs = []\n",
        "            for param, (default, range_) in parameters.items():\n",
        "                if isinstance(range_, list):  # Dropdown parameter\n",
        "                    parameter_inputs.append(gr.Dropdown(label=param, choices=range_, value=default))\n",
        "                elif range_ is None:  # Boolean parameter\n",
        "                    parameter_inputs.append(gr.Checkbox(label=param, value=default))\n",
        "                elif isinstance(range_, tuple):  # Numeric parameter with a range\n",
        "                    parameter_inputs.append(\n",
        "                        gr.Slider(label=param, minimum=range_[0], maximum=range_[1], value=default))\n",
        "\n",
        "            submit_button = gr.Button(\"Build and deploy Model\")\n",
        "            curl_command_output = gr.Textbox(label=\"API Call\")\n",
        "            execution_output = gr.Textbox(label=\"Execution Output\", interactive=False)\n",
        "\n",
        "            submit_button.click(\n",
        "                build_curl_command,\n",
        "                inputs=[model_name, modelfile_content_input, stop_sequence] + parameter_inputs,\n",
        "                outputs=[curl_command_output, execution_output]\n",
        "            )\n",
        "        with gr.Tab(\"Model Info\"):\n",
        "            with gr.Row():\n",
        "                model_name_input = gr.Textbox(label=\"Model Name\", placeholder=\"Enter model name for details\")\n",
        "                model_info_button = gr.Button(\"Get Model Info\")\n",
        "                model_list_button = gr.Button(\"List All Models\")\n",
        "\n",
        "            license_output = gr.Textbox(label=\"License\", interactive=False)\n",
        "            modelfile_output = gr.Textbox(label=\"Modelfile\", interactive=False)\n",
        "            parameters_output = gr.Textbox(label=\"Parameters\", interactive=False)\n",
        "            template_output = gr.Textbox(label=\"Template\", interactive=False)\n",
        "            model_list_output = gr.Textbox(label=\"List of Models\", interactive=False)\n",
        "\n",
        "            model_info_button.click(\n",
        "                fn=show_model_details,\n",
        "                inputs=[model_name_input],\n",
        "                outputs=[license_output, modelfile_output, parameters_output, template_output]\n",
        "            )\n",
        "\n",
        "            model_list_button.click(fn=list_models, inputs=[], outputs=[model_list_output])\n",
        "        with gr.Tab(\"Public Endpoint\"):\n",
        "            # Button to start the original endpoint\n",
        "            start_endpoint_button = gr.Button(\"Start Public Endpoint\")\n",
        "\n",
        "            # Text box to display the last 2 lines\n",
        "            last_2_lines_output = gr.Textbox(label=\"Last 2 Lines\", interactive=False)\n",
        "\n",
        "            # Set the action for the button click\n",
        "            start_endpoint_button.click(start_endpoint_and_get_last_2_lines, inputs=[], outputs=[last_2_lines_output])\n",
        "\n",
        "            # Button to start the OpenAI proxy endpoint\n",
        "            start_openai_button = gr.Button(\"Start Public OpenAI Endpoint\")\n",
        "\n",
        "            # Text box to display the last 2 lines for the OpenAI proxy endpoint\n",
        "            openai_last_2_lines_output = gr.Textbox(label=\"OpenAI Last 2 Lines\", interactive=False)\n",
        "\n",
        "            # Set the action for the button click\n",
        "            start_openai_button.click(start_openai_proxy, inputs=[], outputs=[openai_last_2_lines_output])\n",
        "\n",
        "            # Button to kill the endpoint\n",
        "            kill_endpoint_button = gr.Button(\"Kill Both Endpoints\")\n",
        "\n",
        "            # Set the action for the button click\n",
        "            kill_endpoint_button.click(kill_endpoint, inputs=[], outputs=[last_2_lines_output])\n",
        "        with gr.Tab(\"LiteLLM-Proxy\"):\n",
        "            # Textboxes for displaying logs and status\n",
        "            litellm_log_output = gr.Textbox(label=\"LiteLLM Log Output\", interactive=False, lines=10)\n",
        "            litellm_kill_status = gr.Textbox(label=\"LiteLLM Kill Status\", interactive=False, lines=10)\n",
        "            polling_status = gr.Textbox(label=\"Polling Status\", interactive=False, lines=10)\n",
        "\n",
        "            # Buttons for starting, stopping, and killing the proxy\n",
        "            with gr.Row():\n",
        "                start_litellm_button = gr.Button(\"Start LiteLLM Proxy\")\n",
        "                kill_litellm_button = gr.Button(\"Kill LiteLLM Proxy\")\n",
        "                start_polling_button = gr.Button(\"Start Polling\")\n",
        "                stop_polling_button = gr.Button(\"Stop Polling\")\n",
        "\n",
        "            # Link the buttons to their respective functions\n",
        "            start_litellm_button.click(\n",
        "                fn=start_litellm_proxy_and_read_log,\n",
        "                inputs=[],\n",
        "                outputs=[litellm_log_output]\n",
        "            )\n",
        "\n",
        "            kill_litellm_button.click(\n",
        "                fn=kill_litellm_proxy,\n",
        "                inputs=[],\n",
        "                outputs=[litellm_kill_status]\n",
        "            )\n",
        "\n",
        "            start_polling_button.click(\n",
        "                fn=start_polling,\n",
        "                inputs=[],\n",
        "                outputs=[polling_status]\n",
        "            )\n",
        "\n",
        "            stop_polling_button.click(\n",
        "                fn=stop_polling,\n",
        "                inputs=[],\n",
        "                outputs=[polling_status]\n",
        "            )\n",
        "\n",
        "\n",
        "    app.launch(share=True, display_in_cell=False)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "Sz86u8kUMDoD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}