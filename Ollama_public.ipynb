{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luxadevi/Ollama-Colab-intergration/blob/main/Ollama_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLLAMA GOOGLE COLAB VERSION\n",
        "\n",
        "Build to run ollama for no resource cost and have it accesed publicly\n",
        "\n",
        "For this we need nat-tunneling running on a publicly exposed server\n",
        "\n",
        "This script includes\n",
        "\n",
        "* Downloading the newest cuda drivers\n",
        "* Installing pciutils to show GPU\n",
        "* Installing ollama\n",
        "* Setting up a tunnel to acces te API\n",
        "* Faking a port being in use for tunnel\n",
        "* Downloading Modelfile to test with, location /content/\n",
        "\n",
        "\n",
        "\n",
        "**IMPORTANT: nat-tunnel needs a port in use to check against**\n",
        "\n"
      ],
      "metadata": {
        "id": "zUsHtYp4j54-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
        "!mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n",
        "!cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
        "!apt-get update\n",
        "!apt-get -y install cuda-toolkit\n",
        "!apt-get -y install cuda\n",
        "!sudo apt install pciutils\n",
        "!lspci\n",
        "!wget https://ollama.ai/install.sh -O install.sh\n",
        "!chmod +x install.sh\n",
        "!./install.sh\n",
        "!wget https://raw.githubusercontent.com/Luxadevi/Ollama-Colab-intergration/main/Modelfile\n",
        "!git clone https://github.com/rofl0r/nat-tunnel.git\n",
        "!wget https://raw.githubusercontent.com/Luxadevi/Ollama-Colab-intergration/main/portfaker.py\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KYZf3hNADs3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tunneling setup\n",
        "\n",
        "* Download: https://github.com/rofl0r/nat-tunnel\n",
        "* At least one port exposed for recieving connections\n",
        "\n",
        "On your the server side (where you recieve the connection):\n",
        "\n",
        "run ``python3 natsrv.py --mode server --secret s3cretP4ss --public 0.0.0.0:7000 --admin 0.0.0.0:8000``\n",
        "\n",
        "Where **--secret** is the code you can set yourself and also will be prompted for in this notebook.\n",
        "\n",
        "Port **7000** is the port that will be publicly available exposing ollama This is not neccesary per se if you want to acces it locally.\n",
        "\n",
        "Port **8000** is the port where youll connect to so you need this one at least forwarded"
      ],
      "metadata": {
        "id": "mMTinOsfyZ3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tunneling\n",
        "Start tunnel application and kill the python app that fakes a in use port"
      ],
      "metadata": {
        "id": "RQ8dWc97tJXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python3 portfaker.py > app.log 2>&1 &\n",
        "\n",
        "import os\n",
        "\n",
        "user_ip = input(\"Enter your IP address to connect to: \")\n",
        "user_secret = input(\"Enter the secret: \")\n",
        "\n",
        "nats_command = f\"python3 /content/nat-tunnel/natsrv.py --mode client --secret {user_secret} --local localhost:11434 --admin {user_ip}:8000 &\"\n",
        "\n",
        "# Run the nats_command\n",
        "os.system(nats_command)\n",
        "print(\"natsrv.py is running in the background.\")\n",
        "\n",
        "!pkill -f \"python3 portfaker.py\"\n",
        "\n",
        "print(\"Succesful killed Fake port.\")\n",
        "print(\"Ready to launch ollama\")"
      ],
      "metadata": {
        "id": "7kREpeH0cQim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running ollama\n",
        "You got 2 options or run ollama in the terminal output or have it running in the background. For debugging and the first stages of this app use the terminal output because its way easier to check whats happening in the back"
      ],
      "metadata": {
        "id": "O4Xf_LlmtdNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!OLLAMA_HOST=0.0.0.0:11434  OLLAMA_ORIGINS=http://0.0.0.0:* ollama serve"
      ],
      "metadata": {
        "id": "CrzCPf32ZiRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the environment variables for ollama\n",
        "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "os.environ['OLLAMA_ORIGINS'] = 'http://0.0.0.0:*'\n",
        "\n",
        "# Touch the output.log file to create it if it doesn't exist\n",
        "touch_command = \"touch output.log\"\n",
        "os.system(touch_command)\n",
        "\n",
        "# Run the ollama serve command and log the stdout to output.log in the background\n",
        "ollama_command = \"nohup ollama serve > output.log 2>&1 &\"\n",
        "os.system(ollama_command)\n",
        "\n",
        "print(\"ollama server is running in the background.\")"
      ],
      "metadata": {
        "id": "uvo71_PEWO25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check status just run"
      ],
      "metadata": {
        "id": "4Hr8foj00zJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl localhost:11434"
      ],
      "metadata": {
        "id": "NDMvOs9WvZM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Footnotes\n",
        "\n",
        "You need the latest drivers and cuda toolkit\n",
        "Usually for a http proxy certain applications will be used but i wanted to make this as easy as possible for the user.\n",
        "\n",
        "A SSH SocksV5 proxy wouldn't work because of Google Colab restrictions.\n",
        "\n",
        "You could however use a socks5 when you escape Colab so your API doesnt have to be exposed.\n",
        "\n",
        "Ollama still closes after a while so keep tabs on it when it doesnt respond."
      ],
      "metadata": {
        "id": "8l8YH5rC03fr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
